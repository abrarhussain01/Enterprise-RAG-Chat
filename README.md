# ğŸ“„ Enterprise RAG Chat

An **enterpriseâ€‘grade Retrievalâ€‘Augmented Generation (RAG) application** that allows users to upload documents (PDF/TXT/DOCX), build a vector database, and chat with an AI model grounded strictly on the uploaded content.

This project demonstrates **productionâ€‘style RAG architecture**, combining LangChain runnables, FAISS vector search, Hugging Face chat models, and a Streamlit UI with proper state management.

---

## ğŸš€ Features

* ğŸ“‚ Dragâ€‘andâ€‘drop document upload (PDF supported, easily extensible)
* â›” Blocking ingestion phase (no queries allowed during indexing)
* ğŸ§  FAISS inâ€‘memory vector database
* ğŸ” Retriever invoked on every query (fresh context)
* ğŸ’¬ Full conversational chat (entire chat history sent each turn)
* ğŸ”— Parallel + sequential LangChain runnable pipeline
* ğŸ¤– Hugging Face **ChatModel** (instructionâ€‘tuned)
* ğŸŒ Streamlit UI with clean session lifecycle
* ğŸ§¹ Exit button to reset chat and state

---

## ğŸ§  Architecture Overview

```
User (Streamlit UI)
   â”‚
   â”œâ”€â”€ Upload Document
   â”‚      â””â”€â”€ Backend builds vector DB (blocking)
   â”‚
   â””â”€â”€ Ask Questions
          â”œâ”€â”€ Retriever (FAISS)
          â”œâ”€â”€ Full Chat History
          â”œâ”€â”€ Context + Question
          â””â”€â”€ Chat LLM
```

### Key Design Principles

* **Retrieval is stateless** â†’ always reâ€‘computed per query
* **Conversation is stateful** â†’ full chat history preserved
* **Grounded answers** â†’ model answers only from retrieved context

---

## ğŸ“ Project Structure

```
enterprise-rag/
â”‚
â”œâ”€â”€ app.py            # Streamlit UI
â”œâ”€â”€ rag.py            # RAG backend logic
â”œâ”€â”€ data/
â”‚   â””â”€â”€ uploads/      # Uploaded documents
â”œâ”€â”€ .env              # Hugging Face token
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ” Environment Setup

âš ï¸ **Important:** You must use **your own Hugging Face API key** to run this application.

Create a `.env` file in the project root:

```env
HF_TOKEN=your_huggingface_token_here
```

> A **readâ€‘only Hugging Face token** is sufficient.

---

## â–¶ï¸ Running the Application

```bash
streamlit run app.py
```

---

## ğŸ§ª Application Flow

### 1ï¸âƒ£ Upload Phase

* User uploads a document
* Backend loads, splits, embeds, and indexes the document
* Chat input is **disabled** during this phase

### 2ï¸âƒ£ Chat Phase

* User asks questions about the document
* For **every query**:

  * Retriever fetches topâ€‘K relevant chunks
  * Full chat history + context is sent to the LLM
  * AI response is generated and stored

### 3ï¸âƒ£ Exit Phase

* User clicks **Exit Chat**
* Chat history and vector store state are cleared
* Application resets for a new document

---

## ğŸ§  RAG Pipeline Details

### Parallel + Sequential Chain

* **Parallel stage**:

  * Passes the question through
  * Retrieves relevant context

* **Sequential stage**:

  * Builds system + history + context prompt
  * Invokes Hugging Face ChatModel
  * Saves AI response to chat history

This ensures:

* High accuracy
* No hallucinations
* Proper conversational continuity

---

## âš ï¸ Error Handling

* Empty or unsupported documents are rejected
* FAISS is never initialized with zero chunks
* Userâ€‘friendly errors shown in UI

---

## ğŸ”® Future Enhancements

* ğŸ“š Multiâ€‘document support
* ğŸ” Source citation in responses
* ğŸ§  Query rewriting for followâ€‘ups
* ğŸ’¾ Persistent vector storage
* ğŸ–¼ï¸ OCR support for scanned PDFs

